# Reddit Data Pipeline Engineering üöÄ
An end-to-end data engineering project that extracts data from Reddit, processes and transforms it through an ETL pipeline orchestrated with Airflow and Celery, and makes it queryable via Amazon Athena and Amazon Redshift using external tables through Redshift Spectrum. The pipeline leverages AWS-native services to store, catalog, and analyze data directly from Amazon S3‚Äîwithout loading it into Redshift itself.

---

## üìå Project Overview

This project demonstrates a complete ETL pipeline integrating:

- **Reddit API** for data extraction
- **Apache Airflow & Celery** for orchestration and task scheduling
- **PostgreSQL** for metadata handling
- **Amazon S3** for raw and transformed data storage
- **AWS Glue** for data cataloging and transformation
- **Amazon Athena** for intitial exploration of data from S3
- **Amazon Redshift via externa schema and table** for further analytics and reporting

---

## üõ†Ô∏è Data Architecture
![image](https://github.com/Chisomnwa/Reddit-Data-Pipeline-Engineering/blob/main/images/reddit_etl_architecture.png)

---
## üõ†Ô∏è Tools & Technologies

| Layer | Tools |
|-------|-------|
| Orchestration | Apache Airflow, Celery, Docker |
| Storage | Amazon S3, PostgreSQL |
| Processing | AWS Glue, Athena |
| Data Warehouse | Amazon Redshift |
| Infrastructure | Terraform |
| Programming | Python (with PRAW, Pandas, NumPy, etc.) |

---

## ‚öôÔ∏è Features

- ‚úÖ Modular Terraform scripts to provision VPC, S3, Glue, Athena, Redshift, IAM, and SSM.
- ‚úÖ Dockerized Airflow with custom Python dependencies.
- ‚úÖ Parameterized and dynamic DAG execution.
- ‚úÖ Glue job and crawler triggering from within Airflow.
- ‚úÖ Raw and transformed data cataloged via AWS Glue crawlers.
- ‚úÖ Data queried via Athena and via external tables (spectrum) in Redshift.

---

## üß™ DAG Workflow

1. **Extract** posts from the `dataengineering` subreddit.
2. **Upload raw data** to S3.
3. **Trigger Glue crawler** for raw data.
4. **Upload Glue script** and **trigger Glue job** to transform data.
5. **Trigger crawler** for transformed data.
6. **Query transformed data** using Redshift Spectrum (no need to load data into Redshift).

---

## üóÇÔ∏è Project Folder Structure and Files Description
Here is an overview of the sub-directories and files. Under the reddit_data_engineering main folder, we have:
**Airflow Directory:**
- Complete workflow orchestration setup with DAGs, ETL scripts, pipelines, and utilities
- Glue jobs for PySpark processing
- Proper separation of concerns (ETL, pipelines, constants)

**my_venv Directory:**
This is the virtual environment folder.

**Terraform Directory:**
- Modular architecture with separate modules for each AWS service
- Each module follows Terraform best practices with the main.tf, variables.tf, and outputs.tf (when necessary).
- Backend configuration for remote state management

### Focusing on the Airflow Directory: Key Components
Within the airflow/ directory of this project, several essential subdirectories, and scripts work together to orchestrate the end-to-end pipeline. Here's a breakdown:

**config/ sub-directory**
- ***airflow.cf***: Auto-generated by Airflow, this file holds default - Airflow configurations like scheduler settings, executor type, and logging behavior. It is rarely edited directly.
- ***config.conf***: A custom configuration file designed for this project. It holds project-specific settings such as S3 bucket names, API keys, file paths, and other credentials. This file is read in Python using configparser.

‚ùóÔ∏èImportant: Add config.conf to your¬†.gitignore to avoid exposing secrets in version control.

üìÅ Best practice: Create a config.conf.example file as a template. This serves as documentation for expected keys and formats, allowing collaborators to configure their local environment without needing your secrets.

**dags/ sub-directory**
- ***reddit_dag.py***: This is the main DAG definition script. It orchestrates the ETL process by chaining tasks together‚Ää-‚Ääfrom data extraction to querying the data via Redshift spectrum.

**data/ sub-directory**
- ***output/ folder***: Stores the extracted Reddit data as CSV files. These are later uploaded to S3 as raw data.

**etls/ sub-directory**
- ***aws_etl.py***: Contains reusable components for AWS-related ETL operations like connecting to AWS, creating an S3 bucket for the raw data if it doesn't exist, and uploading data to S3.
- ***reddit_etl.py***: Implements the logic for extracting data from Reddit using PRAW, transforming it into a tabular format, and saving it locally.

**glue_jobs/ sub-directory***
- ***pyspark_script.py***: This is the ETL script that runs in AWS Glue. It reads raw data from S3, performs data cleaning/transformation using PySpark, and writes the transformed data back to a different S3 location.

**pipelines/ sub-directory**
- ***aws_s3_pipeline.py***: Contains helper functions to
   - Upload the raw CSV Reddit data to the raw/ folder in your data S3 bucket.
   -  Upload the PySpark ETL script to the script S3 bucket used by AWS Glue.
- ***glue_pipeline.py***: Provides functions to
   -  Trigger the Glue crawlers responsible for scanning the raw and transformed data folders.
   -  Trigger the Glue ETL job that reads raw data, performs transformations using PySpark, and saves the cleaned data in the transformed/ folder in S3.
- ***reddit_pipeline.py***: Provides functions to
    - This is the wrapper for Reddit data extraction logic. It chains together all the steps from connecting to Reddit, pulling data, transforming it into tabular form, and writing it to a local CSV file ready for upload.
- ***redshift_pipeline.py***: Provides functions to
   - Validate the creation of the Redshift Spectrum (external) schema by querying system views.
   - Load SQL scripts from a mounted Docker volume, with support for dynamic placeholder substitution, useful for parameterized SQL execution in Airflow tasks.

**sql/ sub-directory**
- ***create_spectrum_schema.sql***: Contains the SQL script for creating an external schema in Redshift. Used when we do not want to load data into Redshift tables directly.
- ***create_table.sql***: Contains SQL DDL statements to create the target Redshift table that will store the transformed Reddit data. (But only necessary if you plan to load data directly into Redshift, rather than querying external tables via Spectrum.)

**utils/ sub-directory**
- ***constants.py***: Centralized constants and config loader for the project. It reads from config.conf and provides clean access to settings across different scripts.
---

## üì¶ Setup Instructions

> ‚ö†Ô∏è Ensure you have AWS credentials configured (`aws configure`) and Docker installed.

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-username/Reddit-Data-Engineering.git
   cd Reddit-Data-Engineering

2. **Set up Python virtual environment**
   ```bash
   python -m venv my_venv

   source my_venv/bin/activate  # or my_venv\Scripts\activate on Windows

3. **Install dependencies**
   ```bash
   pip install -r airflow/requirements.txt

4. **Rename the configuration file and the credentials to the file.**
   ```bash
   mv config/config.conf.example config/config.conf

3. **Build and run Airflow in Docker**
   ```bash
   cd airflow
   docker compose up -d

4. **Access Airflow UI**
   ```bash
   http://localhost:8080
   Username: airflow
   Password: airflow

---

## üìç Future Improvements
- Add tests under the tests/ folder

- Build CI/CD for Terraform and DAG deployment

- Integrate monitoring (e.g., Prometheus or AWS CloudWatch)

- Add unit and integration tests

<!--
## üìñ Medium Article
üìñ Medium Article
üëâ Check out the full walkthrough in the accompanying Medium article: [[Link here]](https://medium.com/@chisomnnamani/building-a-reddit-data-pipeline-220811ed16fa)
-->
